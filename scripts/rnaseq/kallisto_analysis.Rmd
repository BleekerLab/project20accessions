---
title: "Analysis of transcript abundance using Kallisto software (Lior Pachter, Berkeley, USA)"
author: "Marc Galland"
date: "`r Sys.Date()`"
output:
    html_document:
        number_sections: yes
        toc: yes
        toc_depth: 2
        keep_md: true
params:
  datadir: "/Users/mgalland/SURFdrive/trichome_team/06.results_from_xp/RNA-Seq/20161118_kallisto/kallisto"
  info: "/Users/mgalland/SURFdrive/trichome_team/06.results_from_xp/RNA-Seq/20161118_kallisto/info.txt"
  resdir: "~/SURFdrive/trichome_team/11.analysis/candidate_genes/20170314_Schm_IL/"
  genes: "/Users/mgalland/SURFdrive/trichome_team/11.analysis/candidate_genes/20170314_Schm_IL/targets.txt"
  accession2species: "/Users/mgalland/SURFdrive/trichome_team/06.results_from_xp/RNA-Seq/20161118_kallisto/accession2species.txt"

---
From mRNA-Seq files, reads are pseudoaligned to a custom reference (defined in the configuration file).

Scaled counts are generated and normalized (similarly to DESeq size factors).

Boxplots of gene expression are generated. 
Principal Component Analysis are generated. 

For more information on Kallisto, see: <https://pachterlab.github.io/kallisto/about.html>
FOr more information on Sleuth, see: <http://pachterlab.github.io/sleuth/>

# Setup
```{r Set global options,echo=FALSE}
knitr::opts_chunk$set(cache=FALSE,warning=FALSE,message=FALSE,fig.align="center") 
```

## load librairies
```{r}
library(ggplot2)
library(dplyr)
library(sleuth)
library(yaml)
library(reshape2)
library(ggrepel)
library(factoextra)
library(ggdendro)
library(corrplot)
library(parallel)
```

## allow multi-threading
```{r}
# set the number of available cores to 4
options(mc.cores = 4L)
```

## Load configuration file and select result directory
```{r, warning=FALSE}
#config = yaml::yaml.load_file("kallistoconfig.yaml")
```

## Create result directories (one per type of data)
```{r Create result directories}
# Assign directory names to variables
resdir = params$resdir
boxplotdir = paste(resdir,"boxplots",sep = "")
pcadir = paste(resdir,"pcadir",sep="")

# create directories
dir.create(boxplotdir,showWarnings = F)
dir.create(pcadir,showWarnings = F)
```

## Table of target transcripts 
```{r Display table of transcripts,echo=FALSE,warning=FALSE,message=FALSE} 
# Transcripts of interest (with solyc numbers)
base_dir = params$datadir
transcripts = read.delim(params$genes,header = T,stringsAsFactors = F,quote = "")
colnames(transcripts)[1] = "target_id"
colnames(transcripts)[2] = "name"

# for plot titles
targets = transcripts$target_id
names = transcripts$name
```

## Correspondence between accessions and species
```{r,echo=FALSE,warning=FALSE,message=FALSE}
# Accessions to species
accession2species = read.delim(params$accession2species,stringsAsFactors = F,header=T)
```

## Expression unit to plot
```{r Expression unit}
exprUnit = "est_counts"
```

# analysis based on kallisto data

## Get kallisto normalized expression for relevant transcripts
```{r Load kallisto files and create Sleuth object}
# Configurate file for Sleuth
sample_id = dir(base_dir)
kal_dirs <- sapply(sample_id, function(id) {file.path(base_dir, id)})
s2c = read.table(params$info,header=T,stringsAsFactors=FALSE)
s2c = dplyr::mutate(s2c,path=kal_dirs)

# Creation of the sleuth object 
so = sleuth_prep(s2c, ~ accession)

# Extraction of the normalized expression data 
df = kallisto_table(so,normalized = T,use_filtered = T)
df.notNormalized = kallisto_table(so,normalized=F,use_filtered = T)

# melt
m_df = melt(df,id.vars=c("sample","accession","target_id"),measure.vars = c(exprUnit))
```

## Sample clustering
### Correlation heatmap
```{r sample correlation heatmap}
# calculate correlation between samples
matrix4cor = dcast(m_df,target_id ~ sample)
corMat = cor(matrix4cor[-1],method = "pearson")

# sample correlation heatmap
# method -- accepted values are "number","circle","square","shade"...
# type -- accepted values are "full" or "upper"/"lower" (for symetric matrixes)
corrplot(corMat,method="circle",type="full",order="hclust",tl.cex=0.5)
svg(filename = file.path(resdir,"sample_correlation_heatmap.svg"))
corrplot(corMat,method="circle",type="full",order="hclust",tl.cex=0.5)
dev.off()
```

### Hierarchical clustering
Based on Pearson correlations between samples. 
```{r hierarchical clustering}
# calculate distance matrix
distMatrix = as.dist(1 - corMat)

# hclustering
# extract data from hierarchical cluster
hcdata = dendro_data(model = hclust(distMatrix,method = "average"))
labs = label(hcdata);colnames(labs)=c("x","y","sample")
labs = dplyr::left_join(labs,s2c[,1:2],by="sample")

# plot
p = ggplot(hcdata$segments) +
  geom_segment(data = hcdata$segments,aes(x,y,xend = xend,yend = yend)) +
  geom_text(data=labs,aes(x=x,y=0,label=labs$sample,colour=labs$accession),size = 3,hjust=1) +
  coord_flip() +
  scale_y_continuous(breaks = NULL,limits = c(-0.05,max(hcdata$segments$y) + 0.1*max(hcdata$segments$y))) +
  theme(
    axis.text.x=element_blank()
    )
print(p)
```


## Boxplots

### Regenerate a dataframe with individual values
```{r}
# keep only transcript of interest
m_df = filter(m_df,target_id %in% targets)
# add transcript name to melted df
m_df = dplyr::left_join(x = m_df,y = transcripts,by="target_id")
#  adding species name and transcript name / annotation
m_df = dplyr::left_join(x = m_df,y = accession2species)
```

### Boxplots 
```{r Boxplots,echo=FALSE}
######################################
## Boxplots ordered by accession names
######################################
# empty it if old boxplots are in there
unlink(boxplotdir,force = T)

# Legend for y axis
if (exprUnit == "est_counts") {
  yAxisLegend = "Normalized expression (counts)"
  } else if (exprUnit == "tpm") {
    yAxisLegend = "Normalized expression (transcript per million)"
  } else
    print("please choose betwen 'tpm' and 'est_counts' as expression units")

# Boxplots
for (i in seq_along(1:length(targets))){
  # create a temporary df with only one transcript
  tmp_df = dplyr::filter(m_df,target_id == targets[i])
  tmp_df = dplyr::arrange(tmp_df,species) # arrange df by species
  tmp_df$accession = factor(tmp_df$accession,levels=unique(tmp_df$accession)) # convert2factor + reorder levels
  # makes a title and a y-axis legend that depends on unit chosen
  plotTitle = paste(targets[i],names[i],sep=" ")
  # check if there is anything at all in the dataframe (some genes do not pass the test)
  if (dim(tmp_df)[1] == 0){
    cat("")
    cat("This gene did not pass the filters: ",targets[i])
    cat("")
} else {
      # boxplot
      p = ggplot(tmp_df,aes(x = accession,y = value,fill=species)) +
      geom_boxplot() + 
      stat_summary(fun.y="mean",geom="point",shape=23,size=2,fill="white") +
      theme(axis.text.x = element_text(angle=30,hjust=1,vjust=1)) + 
      ggtitle(paste(targets[i],names[i],sep=" ")) +
      theme(plot.title = element_text(size=18)) +
      labs(x = "Accession",y= yAxisLegend)
      ggsave(filename = file.path(boxplotdir,paste(targets[i],"_",names[i],".png",sep = "")),width=7,height=5,dpi = 400)
      print(plotTitle)
      print(p)
    }
  }
```

### Computes expression median (unit = scaled counts) and writes to file
```{r Output desired csv files}
# calculate median expression value
agg_df = m_df %>%
  dplyr::group_by(accession,target_id) %>%
  dplyr::summarize(value = median(value))

# long to wide
w_df = reshape2::dcast(agg_df,target_id ~ accession)
w_df = dplyr::left_join(x = w_df,y = transcripts,by = "target_id")
# output csv file
write.table(x = w_df,file = file.path(resdir,"kallisto_expression_values.txt"),quote = F,sep = "\t",row.names = F,col.names = T)
```

## PCA in the manner of sleuth::plot_pca
Makes several plots related to the Principal Component Analysis 
### Compute PCA (unscaled!)
```{r PCA on all genes}
# get matrix of values
mat = sleuth:::spread_abundance_by(
  abund = so$obs_norm_filt,
  var = exprUnit,
  which_order = so$sample_to_covariates$sample)

# Calculate PCA
pca_res <- prcomp(mat,scale. = F,center = F)
```

### Screeplot
```{r Screeplot}
# y-axis limit
max_variance_explained = max(summary(pca_res)$importance[2,])*100

# Plot proportion of variance explained by the 10 first PCs
p = fviz_screeplot(pca_res) +
  labs(x = "Principal components",y = "Proportion of variance explained (%)") +
  scale_y_continuous(limits = c(0,max_variance_explained+0.10*max_variance_explained))
ggsave(filename = file.path(pcadir,"screeplot.png"),plot = p,width = 7,height = 5,dpi = 400)
````

### Plot variable PCA (samples) on combinations of PCs
```{r}
# extract principal component loadings (columns are eigenvectors) 
pcs <- sleuth:::as_df(pca_res$rotation[,c(1L,2L,3L)])
pcs$sample <- rownames(pcs)
rownames(pcs) <- NULL
pcs <- dplyr::left_join(pcs, so$sample_to_covariates, by = "sample")

# legend for each PC
legendPC1 = paste("PC1",paste(round(get_eig(pca_res)$variance.percent[1],digits = 0),"%",sep = ""),"of total variance explained")
legendPC2 = paste("PC2",paste(round(get_eig(pca_res)$variance.percent[2],digits = 0),"%",sep = ""),"of total variance explained")
legendPC3 = paste("PC3",paste(round(get_eig(pca_res)$variance.percent[3],digits = 0),"%",sep = ""),"of total variance explained")
````
#### PC1 and PC2
````{r}

# Plot variables on PC1 and PC2
p = ggplot(pcs,aes(PC1,PC2,colour=accession,label=sample)) +
  geom_point(aes(PC1,PC2),size=3,alpha=0.8) +
  geom_text_repel(size=3) +
  labs(x = legendPC1,y = legendPC2)
ggsave(filename = file.path(pcadir,"pcaPC1vsPC2.png"),plot = p,width = 9,height = 5,dpi=600)
````
#### PC1 and PC3
````{r}
# Plot PCA on PC1 and PC3
p = ggplot(pcs,aes(PC1,PC3,colour=accession,label=sample)) +
  geom_point(aes(PC1,PC3),size=3,alpha=0.8) +
  geom_text_repel(size=3) +
  labs(x = legendPC1,y = legendPC3)
print(p)
ggsave(filename = file.path(pcadir,"pcaPC1vsPC3.png"),plot = p,width = 9,height = 5,dpi=600)
```
#### PC2 and PC3
````{r}
# Plot PCA on PC2 and PC3
p = ggplot(pcs,aes(PC2,PC3,colour=accession,label=sample)) +
  geom_point(aes(PC2,PC3),size=3,alpha=0.8) +
  geom_text_repel(size=3) +
  labs(x = legendPC2,y = legendPC3)
print(p)
ggsave(filename = file.path(pcadir,"pcaPC2vsPC3.png"),plot = p,width = 9,height = 5,dpi=600)
```

### Plot individuals (transcripts) on PCs
```{r gene contribution to each PC}
# limits on PC1
limitsPC1 = c(
  summary(pca_res$x[,1])[[1]] + 0.1*summary(pca_res$x[,1])[[1]],
  summary(pca_res$x[,1])[[6]] + 0.1*summary(pca_res$x[,1])[[6]]
)
limitsPC2 = c(
  summary(pca_res$x[,2])[[1]] + 0.1* summary(pca_res$x[,2])[[6]],
  summary(pca_res$x[,2])[[6]] + 0.1* summary(pca_res$x[,2])[[6]]
)

# extract top 5 transcript that contributes the most to PC1
contribs = as.data.frame(get_pca_ind(pca_res)$contrib)
contribs$genes = row.names(contribs)
topPC1 = head(row.names(contribs[order(contribs$Dim.1,decreasing = T),]),n = 10)

# highlight gene of interest
#transcriptOfInterest = config$pca$rna2highlight

# plot individuals on PC1 and PC2
coords = as.data.frame(pca_res$x[,1:2])
coords$transcripts = row.names(coords)
coords = filter(coords,transcripts %in% topPC1)                     
p = fviz_pca_ind(pca_res,axes = c(1,2),repel = F,labelsize = 4,addEllipses = F,title = NULL,label = "none") +
  scale_x_continuous(limits = limitsPC1) +
  scale_y_continuous(limits = limitsPC2) +
  geom_point(data=coords,aes(PC1,PC2),colour="red",size=3) +
  geom_text_repel(data=coords,aes(PC1,PC2),colour = "red",label=coords$transcripts) +
  labs(x = legendPC1,y = legendPC2)
print(p)
ggsave(filename = file.path(pcadir,"pca_transcripts_PC1PC2.png"),plot = p,width = 7,height = 5,dpi = 600)
```

# Sleuth analysis
## Differential analysis
```{r Sleuth differential analysis,echo=FALSE,message=FALSE}
# Fit the complete model
so = sleuth_fit(so,fit_name = "full")
so = sleuth_fit(so,formula = ~ 1,fit_name = "null")
so = sleuth_lrt(so,null_model = "null",alt_model = "full")
so = sleuth_results(so,"null:full","lrt")
```

# Save sleuth object (for use with a Shiny app for instance)
```{r}
date = strsplit(format(Sys.time()),split = " ")[[1]][1]
resdir = saveRDS(so,file.path(resdir,paste(date,"sleuth_object.rds",sep = "_")))
```

# Get normalization factors
```{r get norm factors}
mat4normfactors = dcast(df.notNormalized,formula = target_id ~ accession,fun.aggregate = mean,value.var = "est_counts")
normFactors = norm_factors(mat4normfactors[,2:ncol(mat4normfactors)])
normFactors = as.data.frame(normFactors)
write.table(normFactors,file.path(resdir,"normFactors.txt"),sep="\t",quote=F,row.names = T)
```

# Session info
```{r}
sessionInfo() 
```
